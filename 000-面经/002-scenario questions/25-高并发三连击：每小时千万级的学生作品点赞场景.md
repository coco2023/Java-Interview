本文我们继续来讲高并发解决方案，这次的业务场景为 —— 每小时千万级的学生作品点赞场景。

话不多说，我们直接切入正文。



## 业务场景 & 对应指标

某大型在线教育公司，其核心业务是 1v1 英语素质教育，当前注册学员的数量达到了百万量级。

公司有如下的一个业务场景，每个单元的课程上完后，就会解锁一个绘画 + 英文创作的额外环节，学生进行创作后并将作品进行上传展示，外教老师会对学生的作品进行点评和打分，与学生进行互动交流。

学生作品如下图所示：

<p align=center><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/854cbbbda9584c8bbcf9862c98e632f8~tplv-k3u1fbpfcp-jj-mark:0:0:0:0:q75.image#?w=600&h=367&s=309330&e=png&b=b0b2b5" alt=""  /></p>



此外，公司的运营团队会收集 100 个外教老师打分最高的学生作品，再进行一个朋友圈的点赞排行榜的活动，榜单的前十名会分别赠予不同的礼物。

活动的规则是，微信朋友圈的任何人都可以进行参与，但只能给一个学生的作品点赞，且不能对一个作品重复点赞。

另外还有两点：

1. 该点赞场景不允许出现上文[《万级 TPS 的 1v1 约课场景》](https://juejin.cn/book/7331654939661795339/section/7344759586257109004)的 “约课中” 的中间状态，点赞操作需要实时生效。

2. 该点赞场景可以查看到点赞人的微信名称。这样就意味着，我们不能仅仅存储一个点赞次数，同样需要存储点赞明细数据，这也是本文中的最大技术难点。

这是一个绝对意义上的高并发场景，每次活动的第一个小时，都会收到几千万的点赞数，每秒钟的平均点赞数 8000+，峰值可以达到 15000+。

活动上线之初，由于研发团队并没有在技术方案上做好充足准备，系统直接被这突如其来的巨大流量给打挂了。




## 思考和落地路径

还记得我们在上文[《高并发三连击：万级 TPS 的 1v1 约课场景》](https://juejin.cn/book/7331654939661795339/section/7344759586257109004)中所论述的观点吗？

高并发写操作的底层解决思路：瞬时流量洪峰通过消峰（MQ 消峰）的方式解决，持续的高并发流量，则通过分散热点（分库分表、单元化）的方式进行解决。

上文中的约课场景只有十分钟的流量高峰期，而本文的活动点赞场景的流量高峰期，则需要持续整整一个小时。

因此，该场景更加 match 于持续的高并发流量的业务场景，需要通过“分库分表”中的“水平拆分”方案来解决存储点赞明细数据的问题。

水分拆分需要考虑的因素有三个：

-   查询操作中的路由因素；
-   插入操作中的热点分散因素；
-   技术方案的复杂度因素。

在这三个考虑因素中，我们当然要选择 “插入操作中的热点分散因素”，这本来就是我们本文中要解决的问题。



### 水平拆分策略选型

再来回顾一下四种水平拆分策略，其中包括：`Range 拆分`、`Hash 取模拆分`、`Hash 取模 + Range 拆分`和 `Hash 取模 + 映射表拆分`。

Range 拆分：

<p align=center><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/807642f5acec4bf0beba7946c5662808~tplv-k3u1fbpfcp-jj-mark:0:0:0:0:q75.image#?w=521&h=353&s=22315&e=png&b=ffffff" alt=""  /></p>



Hash 取模拆分：

<p align=center><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/6a69605ef05c4b7bb89868b3f85ab0e5~tplv-k3u1fbpfcp-jj-mark:0:0:0:0:q75.image#?w=530&h=373&s=31676&e=png&b=ffffff" alt=""  /></p>


Hash 取模 + Range 拆分：

<p align=center><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/78d0bafd62e749d7a54e854a9c77aa2e~tplv-k3u1fbpfcp-jj-mark:0:0:0:0:q75.image#?w=689&h=355&s=31026&e=png&b=fefefe" alt=""  /></p>



Hash 取模 + 映射表拆分：

<p align=center><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/d302d1a9719a424bb8cc7994552b86a6~tplv-k3u1fbpfcp-jj-mark:0:0:0:0:q75.image#?w=689&h=392&s=32447&e=png&b=ffffff" alt=""  /></p>



> 如果大家对这部分知识有些生疏了，请移步本小册的第 9 章[《关于分库分表，这些知识你都了解吗？》](https://juejin.cn/book/7331654939661795339/section/7333971459905159209)。

在这四种方案中，我们用排除法先将 `Range 拆分`的策略排除掉，因为该策略的缺点是：热点数据全部集中在一张表中，在高并发的场景下容易产生性能瓶颈。

在剩下的带 Hash 取模拆分的三种策略中，我们继续排除掉 `Hash 取模 + 映射表拆分`的策略，因为该场景中不会涉及到一对多的情况。

在 `Hash 取模拆分`和 `Hash 取模 + Range 拆分`的两个策略中，我们选择了后者。

原因在于，我们希望通过 Range 拆分的方式，使单表数据量级得到很好的控制，而不是随着时间的推移，表中的数据越累积越多，哪怕是用户不再使用的冷数据。

在该场景下，已结束的点赞活动中的点赞数据，在系统中是不给用户提供查询入口的，所以我们可以将这类数据以半归档的方式进行存储。



### Sharding Key 选型

水平拆分策略定了之后，接下来我们就需要进行 Hash Sharding Key 的选型了。

**1. 通过活动 ID 拆分**

我们首先想到、并首先排除的就是通过活动 ID 进行分库分表了。

首先想到的原因是，通过活动 ID 进行分表，从研发效率上来看绝对是最优解。对于某次活动中的所有数据库操作，都可以在一个库表中完成，把分库分表所带来的研发效率影响几乎降低为 0。

而首先进行排除的原因是，通过活动 ID 进行分表，本次活动的点赞热点数据仍然全部打在一个库表上，并没有用到 Hash 取模拆分策略的优势。

<p align=center><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/649623915b8d426f944dcdfde8ef2924~tplv-k3u1fbpfcp-jj-mark:0:0:0:0:q75.image#?w=720&h=386&s=93914&e=png&b=ffffff" alt=""  /></p>



**2. 通过作品 ID 拆分**

如文中所说，每次活动会收集 100 个外教老师打分最高的学生作品，因此按照作品 ID 进行拆分，可以有效地将热点数据进行分散了。

但会不会存在这样一种情况呢？这 100 个作品中的其中一个是鹤立鸡群般的存在，所有的点赞数都集中在这一个作品上。

从跟风心理来讲，我认为是有可能的。

<p align=center><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/c302e7032ef2407fb0c4f454d8986211~tplv-k3u1fbpfcp-jj-mark:0:0:0:0:q75.image#?w=720&h=383&s=102491&e=png&b=ffffff" alt=""  /></p>


**3. 通过点赞明细 ID 拆分**

毫无疑问，这种方式可以完全将热点数据进行分散了。

但需要注意的是，一旦使用这种方式，不要分出太多的库表，不然在进行数据聚合的时候，会影响研发效率和过度消耗系统硬件资源。在该场景下，我们只分了十个库，每个库中只有一张非归档的明细表，因为其瓶颈点更多地在于磁盘的吞吐量，并不是表中的各种锁。

**分库**：当数据库服务器的硬件资源成为瓶颈的情况下，可以选择分库方案，比如：CPU 使用率 100%，IOPS 和 Load 过高，网卡被打满，等等。

**分表**：当数据库服务器的硬件资源不是瓶颈，而数据库中的锁成为瓶颈的情况下，可以选择分表方案。虽然 MySQL 中的锁有很多，但除了全局锁之外，其他类型的锁都可以通过分表来缓解锁冲突的。

<p align=center><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/fd75ff0c5c214b6696926213cbde3343~tplv-k3u1fbpfcp-jj-mark:0:0:0:0:q75.image#?w=728&h=395&s=42366&e=png&b=fefefe" alt=""  /></p>


### 整体解决方案

**`整体解决方案 = 点赞排行榜汇总数据 + 点赞明细数据落库`**，后者我们已经通过 Hash 取模（点赞明细 ID 拆分） + Range 拆分的方式解决了，那只剩下前者了。

前者的点赞排行榜解决方案，当然毫无疑问地用 Redis Zset 了，每秒钟几千、上万个点赞计数的并发请求根本不在话下。

Zset（SortedSet），是 Set 的可排序版，是通过增加一个排序属性 score 来实现的，适用于排行榜和时间线之类的业务场景。 

如下图所示，这里的 score 属性对应的是销售额：

<p align=center><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/6ca0c670559640a7b53b85c02ad0229c~tplv-k3u1fbpfcp-jj-mark:0:0:0:0:q75.image#?w=680&h=362&s=17728&e=webp&b=ffffff" alt=""  /></p>


在 Redis 7 中，Zset 用了三种数据结构来进行底层实现，分别是 skiplist（跳表）、dict（哈希表）和 listpack。

其具体运用规则如下： 

- 当 ZSet 元素个数小于 128（zset_max_listpack_entries 的默认值），并且元素值小于 64 字节（zset_max_listpack_value 的默认值）的时候，使用 listpack 作为底层结构，以节省空间。

- 否则使用 skiplist + dict 作为底层结构，以提升效率。

说到这里，我们的学生作品排行榜的整体实现方案也就出来了，**通过 `Redis Zset` 解决点赞排行榜的计数功能，并通过 `Hash 取模（点赞明细 ID 拆分） + Range 拆分`的方式进行点赞明细落库**。



<p align=center><img src="https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/548d704a6c7e46588a48ee10c595a277~tplv-k3u1fbpfcp-jj-mark:0:0:0:0:q75.image#?w=730&h=409&s=75356&e=png&b=ffffff" alt="image.png"  /></p>



## 总结

在本文中，我们以两步走的方式分别解决了 “点赞排行榜汇总数据 + 点赞明细数据落库” 的问题，并带着大家重温了分库分表的相关内容，以及简单地普及了 Redis Zset 的一些知识。

该解决方案是我在生产环境中验证过的，确保无坑，请大家在工作和面试中的技术解决方案中放心使用。