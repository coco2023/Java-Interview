相对资深一些的工程师，在技术面试过程中，经常会被面试官问到：“在你的项目中，如何提升可用性 / 可靠性 / 稳定性？”

“系统可用性”、“系统可靠性”、“系统稳定性”这几个词虽然看起来差不多，但各自的衡量角度还是有所不同的，接下来我先解释一下。

- **系统可用性**（Availability）：高可用的系统，故障时间少，止损快，在任何给定的时刻都可以工作。一般公司对系统可用性的要求在 99.9%～99.99% 之间，即：宕机时长在 50 分钟～500 分钟之间。

- **系统可靠性**（Reliability）：高可靠的系统，故障次数少，频率低，在较长的时间内无故障地持续运行。

- **系统稳定性**（Stability）：在系统可靠性和可用性之上，即降低故障频次和提升止损速度的情况下，要求系统的性能稳定，不要时快时慢。

换言之，不但要求系统尽可能地随时可以提供服务，并且希望系统提供有质量保障的服务。

为了便于大家理解“系统可用性”和“系统可靠性”的区别，举个例子：

-   如果系统在每小时崩溃一毫秒，它的可用性就超过 99.9999%，但它还是高度不可靠的。
-   如果系统从来不崩溃，但每年的圣诞节前后停机两周，它是高度可靠的，但是系统的可用性只有 96%。

我们就以“`构建高可用的系统`”这个问题为例，一般被问到这个问题的时候，候选人都能回答出来几点，比如：限流、降级、熔断之类的。但这种散点式的回答并不能让面试官满意，如果能够**站在全局性的角度上，进行一些结构性归类**，那效果会好很多。

构建一个高可用系统，可以分为三大类策略，即：`减少故障次数`、`降低故障时长`、`缩小故障范围`，而每大类中又包含各自细分小类。

整体如下：


<p align=center><img src="https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/b618bcba094b4f7583ec0c6eb7231f1c~tplv-k3u1fbpfcp-jj-mark:0:0:0:0:q75.image#?w=1117&h=512&s=96079&e=png&b=fffbfb" alt="image.png"  /></p>


本文我们主要集中于第一个“减少故障次数”策略。




## 减少故障次数

往下细分，“减少故障次数”策略又包括`限流`、`防刷`、`超时设置`、`系统巡检`和`故障复盘`这常见的五大方法，下面我们逐一讲解。


### 限流

系统外部的请求流量是不受控的，有可能会在某个时间点，由于某个外部因素，导致了流量激增。

而限流的目的，则是要**保护系统不被超出其处理能力的请求冲垮，通过拒绝请求的方式，保证系统的可用性**。

在限流工具上，无论是 Guava 的 RateLimiter，还是 SpringCloud Alibaba 的 Sentinel，功能都非常完善好用。

<p align=center><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/2ea1a8e448a040c59b8e5652ad37a4fb~tplv-k3u1fbpfcp-jj-mark:0:0:0:0:q75.image#?w=495&h=381&s=56681&e=png&b=ffffff" alt=""  /></p>



但是，限流的关键点，从来不在工具上，而在于其`阈值设置`和`多级布控`上。

**（1）阈值设置**

-   如果设置限流的阈值高于系统的承载量，那限流动作就等于形同虚设了。
-   如果设置限流的阈值过低，则拦住了一部分本可以正常处理的业务请求，或造成了服务器硬件资源的浪费。

那么问题来了，`如何才能探查到系统合理的限流阈值呢？`

我认为最好的方案是，在系统的业务低峰期，以真实流量回放、并递增加压的方式（1 倍、1.5 倍、2 倍、2.5 倍、3 倍等）在生产环境上进行压测，探查系统所能承载的最大容量，然后将限流阈值设置为其`峰值容量的 50%～70%`。

<p align=center><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/50bc8a71af724a30901816d638008b6a~tplv-k3u1fbpfcp-jj-mark:0:0:0:0:q75.image#?w=329&h=304&s=14736&e=png&b=fefefe" alt=""  /></p>


之所以设置 50%～70%，就是为一些意外情况留有 buffer，比如：两天后代码迭代上线，业务逻辑变复杂了，消耗了更多的硬件资源，或是压测所得到的峰值容量有些偏差，等等。

当然，这里所说的压测，如果是读写混压的话，还涉及到影子库、影子表和路由策略的一些知识点，在这里就不进行展开了。



**（2）多级布控**

我们对 A 系统进行压测，得出来系统的峰值容量为 1000 QPS，按照其 50%～70% 设置系统整体的限流阈值，这样就完全高枕无忧吗？

有些系统是可以的，但有些系统未必。

假如：

我们把 A 系统中的限流阈值设置为 600 QPS，正常情况下，系统中有 90% 的接口耗时在 20ms 以下，另外 10% 的接口耗时则高达 2000ms 以上。

但忽然有一天，耗时 2000ms 以上接口比例，从 10% 增加到了 50%，而耗时在 20ms 以下的接口比例，则从 90% 减少到了 50%。

这种情况下，系统还是会出问题的。

除非，我们在系统整体限流的情况下，对这几个耗时奇高的接口，再 case by case 地单独设置限流阈值。即：**总分多级布控**。

<p align=center><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/acac1c3eaf0c4e1695302fbbcf6cd3fa~tplv-k3u1fbpfcp-jj-mark:0:0:0:0:q75.image#?w=721&h=424&s=30929&e=png&b=ffffff" alt=""  /></p>


除此场景外，如果系统为几个不同的外部公司，或者公司内不同的几个业务线同时提供服务，也需要根据其请求来源进行总分多级布控。




### 防刷

**防刷：通过限制同一时间内单一用户（或 IP）对特定接口的访问次数，以拒绝请求的方式，保证系统的可用性**。

如果说限流，所限制的是正常的用户业务请求的话，那防刷策略，则更多的是防止恶意请求（DDoS 攻击）、不正常请求（工具抢票），以及代码实现问题（接口内重复调用、循环调用）。

防刷策略的话，我认为`越在上层进行拦截处理越好`，优先在 WAF（Web 应用防火墙）进行拦截，其次在 Nginx 进行拦截，最后在应用服务器进行拦截。

<p align=center><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/412d7d9ee04b43dead29b29012bade32~tplv-k3u1fbpfcp-jj-mark:0:0:0:0:q75.image#?w=448&h=341&s=36766&e=png&b=ffffff" alt=""  /></p>



常见解决方案：

1. 通过 WAF 进行 IP 围栏和 DDoS 速率限制。
2. 通过 Nginx 的 `limit_req_zone` 参数，限制单个 IP 的请求处理速率。
3. 通过用户名 + 方法名作为 Redis Key，并把过期时间设置为间隔阈值，限制用户名的请求处理速率。

不过，这种方式适合于业务流程型的大接口（如：下单、约课、查询商品列表），而非 By ID 类的小接口，同时也需要跟上游团队约定规范，以免误杀正常业务使用。





### 超时设置

一般情况下，系统对于下游服务的依赖，分为强弱依赖。

-   强依赖：假定服务 A 依赖于服务 B，服务 B 出现故障不可用时，服务 A 也不可用。
-   弱依赖：假定服务 A 依赖于服务 B，服务 B 出现故障不可用时，服务 A 虽然受到了些许影响，但是仍然可用。

假设如下场景：

当服务 A 请求下游的强依赖服务 B 的某接口，该接口 TP50 的响应时间为 10ms，TP99 的响应时间为 100ms，但 TP999 的响应时间为 5000ms，相当于是处理 TP50 的 500 个请求的时间总和。

> 解释一下：TP99 = 100ms，标识这段时间 99% 的请求执行时间都在 100ms 以内，TP50 和 TP999 也是相同计算策略。

为了防止高并发下，TP99 以上，甚至是 TP999 这种长尾请求对系统造成的影响，都会给下游接口调用设置一个合理的超时时间。

此举是为了牺牲零星接口的处理结果，保证系统中的绝大多数请求不被其所影响，确保业务正常运行。

一般接口的超时时间，会设置在 TP99 和 TP999 之间，比如：TP99 的响应时间为 100ms，则超时时间可以设置为 200ms。

<p align=center><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/b84533e30f2f46efac53717bf8cbf940~tplv-k3u1fbpfcp-jj-mark:0:0:0:0:q75.image#?w=357&h=290&s=16072&e=png&b=ffffff" alt=""  /></p>





### 系统巡检

系统巡检一般是应用在代码上线后，或是系统业务高峰期以前进行的，旨在**提前发现并处理系统中的潜在问题**。

业务高峰期以前进行，适合于业务波峰和波谷比较明显的情况。

举个例子：在线教育类业务，周中的晚上（19 点～21 点）为业务高峰期，那么可以选择 16 点以后禁止发布上线，17 点进行巡检，一旦发现系统问题，还有两个小时的时间进行处理解决。

巡检内容包括：

1. 应用、数据库、中间件服务器的硬件指标，比如：负载、CPU、磁盘、网络、内存、JVM 等。
2. 系统 QPS、TPS、接口响应时间、错误率等。
3. 是否有新增的慢查 SQL，以及 SQL 执行时间和次数等，这点尤为关键。

<p align=center><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/9ffe4bcf6117414a803a786f286a2701~tplv-k3u1fbpfcp-jj-mark:0:0:0:0:q75.image#?w=485&h=353&s=45607&e=png&b=fefdfd" alt=""  /></p>






### 故障复盘

现在市面上故障复盘的方法论很多，比如：5 WHYS 分析法、5W2H 分析法、黄金三问分析法等，其本质都是围绕故障本身去进行深挖，用追根究底的精神去发掘问题的本质，而不是仅仅停留在“开发的时候没有想到”、“测试的时候没有覆盖到”、“巡检的时候遗漏了”等层面。


<p align=center><img src="https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/98975c2ce00b4f219de247b8978fb04c~tplv-k3u1fbpfcp-jj-mark:0:0:0:0:q75.image#?w=732&h=181&s=26656&e=png&b=ffffff" alt="image.png"  /></p>



接下来，根据“重要紧急”、“重要不紧急”两个维度，制定短期和中期 TODO，务必明确执行人以及完成时间，并持续地监督跟进，直到所有的 TODO 全部完成。

另外，TODO 必须是可落地的，而不是“下次开发的时候多思考”、“下次测试的时候多重视”、“下次巡检的时候多注意”之类的口号流。

另外，从“减少故障次数”这个角度来说，故障复盘不但是通过流程规范和技术策略，保证在以后的开发迭代中，**系统不再引入增量的同类问题**，也是一种**由点及面地去清理现有系统中的存量问题**。





## 总结

本文主要将“减少故障次数”的内容整体讲解了一下，旨在尽量将系统故障扼杀在萌芽中，以此构建高可用的系统，包括：限流、防刷、超时设置、系统巡检、故障复盘等手段。

后续的两篇文章中，我们会继续讲解“降低故障时长”和“缩小故障范围”，形成提升系统可用性的三部曲。