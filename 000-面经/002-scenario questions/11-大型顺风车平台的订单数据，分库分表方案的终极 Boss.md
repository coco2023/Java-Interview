在上一章节的“案例一”中，我们从不完善到完善，以渐进优化的方式，跟大家介绍了电商订单数据的分库分表解决方案。我们也可以从中看到，对于当前技术方案进行迭代升级的思考和选型取舍。

在本文中，我们会继续以这样的方式，跟大家介绍一个业务场景更加复杂、也更加小众化的“订单数据”分库分表解决方案。

当然，说的还是数据量大、并发量高、重要性强、查询维度多的订单数据。



## 案例二

国内某大型顺风车出行平台，包含乘客和顺风车主两种角色，目前的日订单稳定在 50 万单左右。

在系统设计之初，技术团队一直在订单数据的分库分表方案上一筹莫展，连续开了好几次会头脑风暴，最终都没制定出相对完美的技术方案。

跟上文的 “案例一” 一样，顺风车案例的订单数据也没有非常明显的冷热字段和大字段，所以并不需要考虑垂直拆分，只需要考虑进行水平拆分即可。

其中的水平拆分的技术难点在于：

- 传统的电商平台只有用户才可以下单，而顺风车则是乘客和车主都可以发顺风车单。
- 一次顺风车交易最多可以拼三个乘客，即：一个车主单可以对应多个乘客单。

当时，技术团队在进行头脑风暴的时候，也同样提出了几种分库分表的方案，我们先看下这几种方案都有哪些缺点，然后再抛出最终的解决方案。



### 1. 多 Sharding Key 分库分表 + Canal + ElasticSearch

提出这个方案的同学是懂分库分表的，直接上《企业 IT 架构转型之道：阿里巴巴中台战略思想与架构实战》一书中的多个 Sharding Key（订单 ID、乘客 ID、司机 ID）分库分表解决方案了。

<p align=center><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/3acb453cddee4dc0b67f8438739f8b96~tplv-k3u1fbpfcp-jj-mark:0:0:0:0:q75.image#?w=697&h=272&s=34596&e=png&b=ffffff" alt=""  /></p>



这个解决方案，既然已经被阿里在真实场景验证过，那一定是非常成熟的方案，直接拿来沿用即可，对吧？

但是，这个方案如何解决乘客和司机缺失的问题呢？

举个例子：如果乘客 A 进行下单了，但暂时没有顺路的司机接他，此时司机 ID 是空的。

那这条订单数据是可以存在于订单数据库中的，因为订单数据库是按照订单 ID 进行分库分表的。这条订单数据同样可以通过 Canal 进行 Binlog 同步，储存于乘客订单库中，因为乘客订单库是按照乘客 ID 进行分库分表的。

但是，这条订单数据连司机 ID 这个 Sharding Key 都没有，它如何通过 Canal 进行 Binlog 同步，储存在司机订单库中呢？

因此，经过真实场景的业务逻辑进行推演，这个方案是不成立的。




### 2. 城市 ID 作为 Sharding Key + 映射表 + Canal + ElasticSearch

其他同学马上提出了第二个方案，即：城市 ID 作为 Sharding Key + 映射表 + Canal + ElasticSearch。

<p align=center><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/426103726c444aefa18e677923db33c2~tplv-k3u1fbpfcp-jj-mark:0:0:0:0:q75.image#?w=645&h=263&s=28510&e=png&b=ffffff" alt=""  /></p>



既然这个业务场景无法保证所有 Sharding Key 同时存在，那干脆用城市 ID 作为单 Sharding Key 的方式就好了。

（1）当乘客下单的时候，我们可以将缺失司机 ID 的订单数据，同时写入到城市订单库的表、映射表和 ElasticSearch 中。当司乘匹配的时候，再将城市订单库的表、映射表和 ElasticSearch 中的司机 ID 补全，并在城市订单库的表中随机生成一个 BatchID，用以标识拼单。

（2）如果司机再拼入另一个乘客时，再将另一个订单数据的城市订单库的表、映射表和 ElasticSearch 中的司机 ID 补全，并将其 BatchID 与步骤 1 中的 BatchID 设置为相同。

（3）然后，当我们需要通过司机 ID、乘客 ID、或订单 ID 进行查询时，都可以在映射表中查到对应的城市 ID，然后再路由到对应的城市订单库表中进行查询即可。

以上步骤是“乘客下单”的业务场景，其同样适用于“司机下单”的业务场景，我们在此就不进行赘述了。

再来说说映射表的问题，我在上文中提到了两个观点：

（1）如果映射表数据量过大，也需要进行分库分表。

（2）单表数据达到几千万，需要进行分库分表，因为单条用户记录大小为 1kb，高度为 3 的 B+ 树可以存储两千万行记录，如果单条用户记录大小为 512b，高度为 3 的 B+ 树可以存储四千万行记录。

一旦超出范围的话，B+ 树的高度就会从 3 变成 4，也就是说，By ID 查询的磁盘 IO 次数从 3 次变成 4 次，多增加了一次磁盘 IO，增加了性能消耗。

那推论下来，映射表也是需要进行分表的，但其数据列只有四列，且都是 int 或 bigint 类型，占用空间很小，可以把分表的阈值从几千万数据记录提升至几亿。

这样分表的数量就可以大大减少，看起来这个方案不错，是可以执行落地的。

但有同学又提出了这样的疑问：“对于同城顺风车的业务场景，这确实是个好的方案。但对于跨城顺风车的多乘客场景，即：司机的出发地是秦皇岛，乘客 A 的出发地是北京，乘客 B 的出发地是天津，乘客 A 的目的地是德州，乘客 B 的目的地是济南，司机的目的地是泰安，那城市 ID 应该取哪个？”


<p align=center><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/016ff570ac7b485fa75a974ba578f1e6~tplv-k3u1fbpfcp-jj-mark:0:0:0:0:q75.image#?w=713&h=122&s=27565&e=png&b=fefefe" alt="image.png"  /></p>



确实，这方案不能很好地支持跨城顺风车的业务场景，也是不成立的。



### 3. 订单 ID 作为 Sharding Key + Canal + ElasticSearch

有的同学建议，索性把技术方案简单粗暴化，直接通过订单 ID 作为 Sharding Key 进行分库分表，数据库只支持根据订单 ID 查询，根据司机 ID、乘客 ID，以及其他维度的查询，全部走 ElasticSearch。

<p align=center><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/74bc8ef727674261afbb522e59cc33e2~tplv-k3u1fbpfcp-jj-mark:0:0:0:0:q75.image#?w=664&h=293&s=20659&e=png&b=ffffff" alt=""  /></p>



如上图所示，就是这么简单，并且从多个业务场景去进行度量，确实没有发现任何问题点。

正当一致拍板通过之时，团队内的架构师忽然说道：“我有一个担心点，选择这个方案的话，如果 ElasticSearch 集群挂了，是不是整个业务都不 work 了，而不是仅仅影响一部分功能？”

这确实是个大问题，如果数据库不承接按乘客 ID 查询、按司机 ID 查询，这种核心请求的流量完全依靠 ElasticSearch 来支撑，确实会非常影响系统可用性。

而 C 端系统的可用性，是一种完全不可以忽视的大问题，因此这个方案是不推荐的。



### 4. 多 Sharding Key 分库分表 + 手动多写 + ElasticSearch（最终方案）

经过三轮抛砖引玉之后，现在来讲我们的终极方案，“多 Sharding Key 分库分表 + 手动多写 + ElasticSearch”。

其实，我们再回过头来看方案 1 “多 Sharding Key 分库分表 + Canal + ElasticSearch”，该方案不成立的原因，其实无非是在进行 Canal Binlog 同步的时候，差了乘客 ID 或司机 ID 的一个 Sharding Key，导致数据无法写入而已。

我们可以转换一种思路，多 Sharding Key 进行分库分表，一定要通过 Canal Binlog 同步的方式吗？这种方式虽然足够方便，但是欠缺了一些灵活性。

什么方式更加灵活呢？没错，可以手动多写控制。步骤如下：

（1）乘客下单，将缺少司机 ID 的订单数据写入订单数据库、乘客订单库和 ElasticSearch。

（2）司机接单，将司机 ID 和 BatchID（随机生成，用于拼单场景）在订单数据库、乘客订单库和 ES 中补全，并将订单数据完整写入司机订单库。

![](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/125674fa22224e579137cb1b8c00ccac~tplv-k3u1fbpfcp-jj-mark:0:0:0:0:q75.image#?w=715&h=404&s=56683&e=png&b=ffffff)



（3）如果司机再拼入另一个乘客时，将司机 ID 和 BatchID（与步骤 2 中的 BatchID 设置为相同）在订单数据库、乘客订单库和 ES 中补全，并将订单数据完整写入司机订单库。

（4）当我们通过乘客 ID 进行查询时，可以在乘客订单库查到完整的订单记录；通过司机 ID 进行查询时，可以在司机订单库查到完整的订单记录；通过订单 ID 查询，则直接查询订单数据库即可，其他维度的查询走 ElasticSearch。

这样的话，即使 ElasticSearch 集群挂了，也不会影响核心业务链路的正常运行。

至此，我们确定该方案为最终方案。



## 总结

在“分库分表”这三篇文章中，我们首先把涉及到分库分表的技术点全部过了一遍，其中包括：单表几千万数据需要分库分表的理论依据、分库和分表的各自适用场景、水平拆分和垂直拆分的概念、以及分库分表的难点解析，算是进行前置技术储备。

然后，我们又通过两个案例，从不完善到完善，以渐进优化的方式，跟大家介绍了复杂场景的分库分表解决方案。

至此，分库分表三部曲已经全部讲完了。