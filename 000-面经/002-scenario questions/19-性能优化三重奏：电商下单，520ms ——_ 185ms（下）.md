在上文中，我们介绍了五大性能优化方向和对应的 16 种优化策略，以及梳理了回答“系统性能优化”问题的四个步骤，即：按照系统的业务场景及指标 ——> 性能问题的对应原因 ——> 思考和落地路径 ——> 优化后的可量化结果。

接下来，我们还举了一个电商下单的场景的真实案例，已经带着大家梳理了“前置校验逻辑”的性能优化，而“后置通知逻辑”和“核心下单逻辑”，我们继续在文中进行详述。



## 落地路径


### 后置通知逻辑

如下图所示，后置处理逻辑共包含三个步骤，整体耗时为 25ms + 35ms + 30ms，也就是 90ms。

<p align=center><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/b071b995353a499c956d4c85e28ac7d0~tplv-k3u1fbpfcp-jj-mark:0:0:0:0:q75.image#?w=566&h=293&s=35723&e=png&b=fefefe" alt=""  /></p>



这里说的结果相关性是指，即使对三个下游系统（物流系统、结算系统和用户中心）通知不成功，仍然不会影响用户下单成功的结果。

相反，在用户下单成功的情况下，我们可以通过补偿机制，持续对三个下游系统进行通知，完成系统业务逻辑的完整性和数据的最终一致性。

那么这种与结果相关性不大，且相对较为耗时的业务步骤，我们就可以通过`异步化`的方式进行优化，从而减少用户的等待时间，提升用户体验。

如下图所示：

<p align=center><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/02d92d20208b4b0985f1222fdac07dd9~tplv-k3u1fbpfcp-jj-mark:0:0:0:0:q75.image#?w=599&h=332&s=46725&e=png&b=fefefe" alt=""  /></p>



有人可能会问，Kafka 从生产者到 Broker，真的快到只需要 5ms 吗？

Kafka 官方文档原话是这样说的，“Kafka 每秒可以处理几十万条消息，延迟最低只有几毫秒”。这里所说的延迟是指，该消息从生产者发送，到消息被消费者接收之间的一个时间差。

当然，如果消息从生产者发送，到消息被消费者接收只需要几毫秒的话，那本身在生产者端的耗时会更短。

其原因在于，Kafka 生产者的消息发送动作，是由`主线程`和 `Sender 线程（发送线程）`相互配合完成的。



主线程负责消息创建，然后会依次经过拦截器、序列化器和分区器后，将消息缓存在消息累加器中。如下图：

<p align=center><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/1522031b7ef84c168d513c54e3b785b5~tplv-k3u1fbpfcp-jj-mark:0:0:0:0:q75.image#?w=710&h=266&s=27564&e=png&b=ffffff" alt=""  /></p>



-   **拦截器**：此处为 onSend 拦截器，可对发送前的消息进行一些通用性的处理操作，如消息过滤、消息内容二次处理等。（该拦截器为可选。）

-   **序列化器：** 将消息对象序列化为字节数组。
-   **分区器：** 决定将消息发送至哪个分区。如果消息键值为 null，分区器默认的话，会使用轮询（Round Robin）算法决定分区归属；如果消息键值不为空，分区器默认的话，对键进行散列（Hash）来决定分区归属。
-   **消息累加器：** 我们每次发送给 Broker 的消息，不是单条发送的，而是打包批量发送的，这样性能会更高些。而消息累加器起到了聚合消息，以供后续发送的作用。如下图所示：

<p align=center><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/cafe2816bd1a4c5c8dc642b5f6ad9f9f~tplv-k3u1fbpfcp-jj-mark:0:0:0:0:q75.image#?w=776&h=357&s=40426&e=png&b=ffffff" alt=""  /></p>


可以看到，消息累加器内部有一个双端队列，以分区为维度，存储消息批次（ProducerBatch），主线程往队列中写入，Sender 线程从队列中读取。

至此，主线程的工作已经全部完成了，其消耗时长也就计算到这里为止了，并不是大部分同学所认知的那样，直到把消息发送到 Broker 才算完成。

最终，我们通过 Kafka 异步化订单⽀付成功的后置业务流程（通知用户中心、通知结算系统、通知物流系统），将下单接口的后置处理逻辑的总时长从 90ms 优化至 5ms，下单接口总时长从 390ms 优化至 305ms，并与这三个服务进⾏解耦。




### 核心下单逻辑

终于开始讲我们的重头戏 —— “核心下单逻辑”了。

如下图所示，核心下单逻辑共包含六个步骤，创建订单 ——> 扣减库存 ——> 扣减优惠券 ——> 扣减积分 ——> 支付 ——> 修改订单状态，整体耗时 230ms。

<p align=center><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/afe1807c5c8443ecb5b209fe0a23615c~tplv-k3u1fbpfcp-jj-mark:0:0:0:0:q75.image#?w=720&h=351&s=68139&e=png&b=fefefe" alt=""  /></p>



在上述的六个步骤中，我们会先创建订单，当订单创建成功后，再通过分布式事务中的 “Saga 模式” 进行扣减库存、优惠券、积分以及支付这四个步骤的操作。

当这四个步骤中有任意一个步骤失败，则进行回滚操作，并将最后一步的订单状态修改为“支付失败”。若这四个步骤全部成功，则将订单状态修改为“支付成功”。

这里需要先跟大家介绍一下 Saga 模式。

在分布式系统中，我们通常会**把一组相关联的事务操作称之为一个 Saga**，这组事务操作共同完成一个完整的业务功能。

拿我们现在这个 case 来说，扣减库存、优惠券、积分和支付这四个步骤就是一个完整的“订单支付”功能，也就是一个 Saga。Saga 中的每步操作都是一个本地事务，且都有它对应的补偿事务。

当出现 Saga 中的某步操作执行失败的情况，就会执行之前所有成功操作所对应的补偿事务进行回滚，保证数据的最终一致性。

<p align=center><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/3106a7b496fa47379ee1aa0d44590f23~tplv-k3u1fbpfcp-jj-mark:0:0:0:0:q75.image#?w=546&h=407&s=29937&e=png&b=fefefe" alt=""  /></p>



我们现在需要思考的是，写操作可以通过“多线程并行”的方式进行优化吗？如果在并行执行的时候，某个步骤执行失败了，那失败后的补偿逻辑如何设计？

答案当然是可以的，代码 demo 如下：

```java
public boolean createOrder() {

    if (!checkOrder()) {
        return false;
    }

    CompletableFuture<Boolean> payOrder = CompletableFuture.supplyAsync(() -> {
        return true;
    });
    CompletableFuture<Boolean> deductCoupon = CompletableFuture.supplyAsync(() -> {
        return true;
    });
    CompletableFuture<Boolean> deductPoint= CompletableFuture.supplyAsync(() -> {
        return true;
    });
    CompletableFuture<Boolean> deductInventory = CompletableFuture.supplyAsync(() -> {
        return true;
    });

    CompletableFuture<Boolean> result = CompletableFuture.allOf(payOrder, deductCoupon, deductPoint, deductInventory)
            .thenApply(res -> {

            if(payOrder.join() && deductCoupon.join() && deductPoint.join() && deductInventory.join()){
                updateOrderStatus("支付成功");
                return true;
            }else{
                if (!payOrder.join()){
                    System.out.println("调用payOrder逆向接口进行回滚");
                }
                if (!deductCoupon.join()){
                    System.out.println("调用deductCoupon逆向接口进行回滚");
                }
                if (!deductPoint.join()){
                    System.out.println("调用deductPoint逆向接口进行回滚");
                }
                if (!deductInventory.join()){
                    System.out.println("调用deductInventory逆向接口进行回滚");
                }
                updateOrderStatus("支付失败");
                return false;
            }
    });
    return result.join();
}
```

  


从上述代码中可以看到，我们把每个步骤都进行了校验，该步骤是否执行成功，如果执行成功了回滚补偿即可。

  


<p align=center><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/7c98275956074470b953c090c49392e8~tplv-k3u1fbpfcp-jj-mark:0:0:0:0:q75.image#?w=588&h=414&s=56223&e=png&b=fefefe" alt=""  /></p>



老规矩，我们还是按照多路并行执行，取其执行路径中的最长耗时的方式来进行计算。

那么，核心下单逻辑优化前的整体执行时长为 230ms，优化后为 20ms + 70ms + 20ms = 110ms。


## 可量化结果

我们来看下下单接口整体优化结果。

- 前置校验逻辑，从 190ms 优化至 60ms；
- 后置处理逻辑，从 90ms 优化至 5ms；
- 核心下单逻辑，从 230ms 优化至110ms；
- 日志打印、对象组装与解析，以及各种逻辑判断，还是按照耗时 10ms 来计算。

那么，下单接口整体耗时为，60ms + 5ms + 110ms + 10ms = 185ms。



## 总结

我们用了两篇文章，先是介绍了五大性能优化方向和 16 种优化策略，随后梳理了回答“系统性能优化”问题的四个步骤，即：`按照系统的业务场景及指标 ——> 性能问题的对应原因 ——> 思考和落地路径 ——> 优化后的可量化结果`。

最后，我们还举了一个电商下单的场景的真实案例，通过`“并行计算”`和`“异步计算”`的方式，一点点带着大家进行性能优化，最终将下单接口的整体耗时从 520ms 优化至 185ms。

希望大家能够对“并行计算”和“异步计算”这两种方式有深刻的理解，以完成各自系统中的性能优化，并能游刃有余地应对面试官。