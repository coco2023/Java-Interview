在上文中，我们把涉及到分库分表的技术点全部过了一遍，其中包括：单表几千万数据需要分库分表的理论依据、分库和分表的各自适用场景、水平拆分和垂直拆分的概念、以及分库分表的难点解析，算是进行前置技术储备了。

而从本文开始，我们会将之前储备的技术理论知识，结合行业内真实案例的解决方案进行详细讲解。

往往我们在聊到分库分表的时候，永远绕不开的一个业务场景就是“订单业务”。究其原因，我认为有如下几点：

1. **数据量大**。一个商品会对应成千上万个订单，因此订单数据要比商品数据大好几个数量级。
2. **并发量高**。尤其是大型电商平台的大促秒杀场景，会给数据库服务器造成很大压力。
3. **重要性强**。订单数据是跟企业营收直接相关的，重要性极强，对数据一致性和丢数据问题是不能容忍的。
4. **查询维度多**。订单数据不仅要支持按照用户和商家维度查询，还要支持客服中心的各种维度查询。

而数据量大、并发度高、重要性强，以及查询维度多，这些都共同提升了订单业务分库分表的难度指数。

因此，我们讲的第一个解决方案，就是`基于大型电商平台订单数据的分库分表解决方案`。




## 案例一

某大型电商平台的业务增长迅猛，日订单量已经由两个月前的 5 万单增长到了 30 万单，订单表中的数据已经达到了 6000 多万。按照业务团队的增量计算方式，三个月后日订单量会达到 50 万。

因此，给订单表制定分库分表方案，则成为当前技术团队最重要且紧急的事情。

由于订单数据并没有非常明显的冷热字段和大字段，所以并不需要考虑垂直拆分，只需要考虑进行水平拆分即可。

按照未来 5 年日订单稳定在 100 万来预估，一年有将近 4 亿的订单数据，5 年是 20 亿，正好可以分为 10 个库，每个库有 10 张表，单表在 5 年后达到 2000 万数据。

目前，技术团队迭代优化了几种分库分表的方案，下面我们逐一讲述。



### 1. 订单 ID 作为 Sharding Key

通过订单 ID 作为 Sharding Key 进行分库分表，其中，以订单 ID 的最后一位数字取模进行分库，以倒数第二位取模进行分表。

如下图所示：

<p align=center><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/1e1b163b8ebe4e8a88f58bec7c009ad6~tplv-k3u1fbpfcp-jj-mark:0:0:0:0:q75.image#?w=696&h=394&s=61128&e=png&b=ffffff" alt=""  /></p>



这个方案的优点是，按照订单 ID 进行分库分表，每个库表中的数据会分散得非常均匀，且热点数据全部散开，不会出现一个数据库读写压力过大导致性能瓶颈，其他数据库却出现资源闲置的情况。

但这个方案很快就被否定了，因为根据订单中心的请求日志分析，大概 55% 的订单查询请求都是根据用户 ID（user_id）进行查询的，而根据订单 ID 进行查询的请求只有 20%。

相当于如果根据订单 ID 进行分表，那么 55% 的根据用户 ID 进行查询的请求需要进行跨库查询，这显然是不行的。




### 2. 用户 ID 作为 Sharding Key

既然根据用户 ID 进行查询的请求比例这么高，那索性换个思路，通过用户 ID 作为 Sharding Key 进行分库分表吧。其中，以用户 ID 的最后一位数字取模进行分库，以倒数第二位取模进行分表。

如下图所示：

<p align=center><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/d620630e0d1f4adbbb31c28fdea4b1a9~tplv-k3u1fbpfcp-jj-mark:0:0:0:0:q75.image#?w=706&h=394&s=60972&e=png&b=ffffff" alt=""  /></p>



这个方案的优点是，通过用户 ID 作为 Sharding Key 进行分库分表，55% 根据用户 ID 进行查询的请求不需要进行跨库查询了。

并且，在用户量级足够多的情况下，每个库表中的数据仍然会分散得非常均匀，且热点数据全部散开，不会出现一个数据库读写压力过大导致性能瓶颈，其他数据库却出现资源闲置的情况。

但问题的影响比例只是被缩小了一些，却依然是存在的，那就是还会有 45% 的查询请求需要跨库查询。

有的同事建议，直接用多堆从库的方式去硬扛剩下的 45% 流量，但立即被否定了。因为无论是查询性能角度，还是硬件成本角度，都存在问题。

我们思考一下，有没有一种方案，既支持以不跨库的方式根据用户 ID 进行查询，也支持以不跨库的方式根据订单 ID 进行查询呢？





### 3. 用户 ID 作为 Sharding Key + 映射表

当然是有的！“用户 ID 作为 Sharding Key + 映射表”，就是“不跨库根据用户 ID 查询 + 不跨库根据订单 ID 查询”的一种方案。

当我们在创建订单的时候，同时将订单 ID 和用户 ID 的对应关系写入映射表中，这样根据订单 ID 进行查询的时候，就可以先到映射表中查询到用户 ID，再通过用户 ID + 订单 ID 查询到对应的订单数据。

<p align=center><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/e3f194adf1a44932b49570da2cc054f3~tplv-k3u1fbpfcp-jj-mark:0:0:0:0:q75.image#?w=224&h=243&s=15339&e=png&b=dbe0f0" alt=""  /></p>



这个技术方案的优点是简单清晰，开发工作量小，缺点是需要多进行一次映射表查询，且映射表的存储也是一种开销。另外，映射表数据量过大，也需要进行分库分表。




### 4. 用户 ID 作为 Sharding Key + 路由键

“用户 ID 作为 Sharding Key + 路由键”，则是“不跨库根据用户 ID 查询 + 不跨库根据订单 ID 查询”的另一种方案。

这种方案的实现方式是，将路由键作为订单 ID 的一部分，这样就不需要存储订单 ID 和用户 ID 对应关系的映射表了。

按照我们的这个业务场景，订单 ID 的算法如下：

- 订单 ID = 雪花算法生成的 ID + 用户 ID 的后两位

如上文所述，我们“通过用户 ID 作为 Sharding Key” 分了 10 个库，每个库有 10 张表，算起来正好是 100 张表，后两位（00～99）恰好够了。

<p align=center><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/48763a84b96842eb8225bf6777bd5aae~tplv-k3u1fbpfcp-jj-mark:0:0:0:0:q75.image#?w=658&h=249&s=57787&e=png&b=fcf9fc" alt=""  /></p>


需要注意的是，以这种方式生成的订单 ID，MySQL 中的 bigint 数据类型已经存不下了，需要用 decimal(21) 来进行存储。

BTW：方案 3、4 确实有效地解决了“不跨库根据用户 ID 查询 + 不跨库根据订单 ID 查询”的问题，还有 20% 的订单查询请求是根据商家 ID（merchant_id）进行查询的，这该如何解决呢？




### 5. 多 Sharding Key 分库分表

《企业 IT 架构转型之道：阿里巴巴中台战略思想与架构实战》一书中讲到，阿里的订单中心是采用三个 Sharding Key（订单 ID、用户 ID、商家 ID）分库分表实现的，且每个 Sharding Key 所对应的库表都是订单的全量数据。

<p align=center><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/70a9be31dc35494b9f24e475512a5260~tplv-k3u1fbpfcp-jj-mark:0:0:0:0:q75.image#?w=715&h=406&s=60880&e=png&b=ffffff" alt=""  /></p>



<p align=center><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/20b5ee184c30452cb4e018cb25b0789c~tplv-k3u1fbpfcp-jj-mark:0:0:0:0:q75.image#?w=716&h=404&s=59645&e=png&b=ffffff" alt=""  /></p>



<p align=center><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/3a04d1b05776433cb068cee47e7e918a~tplv-k3u1fbpfcp-jj-mark:0:0:0:0:q75.image#?w=714&h=406&s=60342&e=png&b=ffffff" alt=""  /></p>



这种方式的优点是，不像上文所讲的映射表那样，需要进行二次查询，在性能上是有所提升的。缺点则是，需要耗费更多的存储空间进行冗余数据存储。

另外，这种方案如果通过程序控制，进行多个库的数据写入和修改就不合适了，最好通过 Binlog 同步工具 Canal 或 DataBus 去进行多个库的数据同步。

<p align=center><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/87ce153d13224262ba0482ce85c74005~tplv-k3u1fbpfcp-jj-mark:0:0:0:0:q75.image#?w=577&h=272&s=26250&e=png&b=ffffff" alt=""  /></p>


这里简单地介绍一下 Alibaba 的开源组件 Canal，其主要用途是基于 MySQL 数据库的增量日志解析，提供增量数据订阅和消费。

Canal 的工作原理是，把自己伪装成 MySQL 从库，接收 MySQL 主库推送过来的 Binlog，Canal 解析 Binlog 并发送到存储目的地。

存储目的地可以是 MySQL 的其他数据库，也可以是 Redis、Kafka、ElasticSearch、Hbase 等。


<p align=center><img src="https://p1-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/0d3c31f8a7e641f4bacf6bcbe7a73bca~tplv-k3u1fbpfcp-jj-mark:0:0:0:0:q75.image#?w=720&h=379&s=63503&e=png&b=fdfdfd" alt="image.png"  /></p>

这样，我们就已经解决了“不跨库根据用户 ID 查询（55%） + 不跨库根据订单 ID 查询（20%） + 不跨库根据商家 ID 查询（20%）”的问题，相当于解决了 95% 订单不跨库查询问题。





### 6. 多 Sharding Key 分库分表 + ElasticSearch

剩下 5% 的订单查询请求是最难解决了，比如：在客服中心系统中，客服人员在进行订单查询的时候，查询项会有十几个，各种查询维度组合多种多样，甚至还需要通过产品关键字进行模糊查询。

这种情况，别说是已经分库分表了，就算在单表中加联合索引都未必能解决问题。这时，我们的订单系统就需要引入 ElasticSearch 了。

相比较于 MySQL 的 B+ Tree 索引，ElasticSearch 的分词器 + 倒排索引机制，更加适合多维复杂查询和全文检索关键字的场景。

<p align=center><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/c74749cd53b94fa8aa40a8a4221a5654~tplv-k3u1fbpfcp-jj-mark:0:0:0:0:q75.image#?w=711&h=292&s=34489&e=png&b=ffffff" alt=""  /></p>


另外，如果引入 ElasticSearch 后，就可以支持全部条件进行搜索查询了，是否还需要“多 Sharding Key 分库分表”了？这又是一个选择与取舍的问题。

选择多 Sharding Key 分库分表不但耗费了更多的存储资源，同时也使技术方案变得更加复杂，似乎有百害而无一利。

但是，这种方案所提供了一种数据冗余，可以有效地提升系统可用性。当 ElasticSearch 不可用的时候，我们依然可以通过“多 Sharding Key 分库分表”方案支持 95% 的查询请求。

最终，技术团队选择了方案 6——“**`多 Sharding Key 分库分表 + ElasticSearch`** ”，作为未来五年订单数据的分库分表方案，也是订单中心系统最为核心的架构设计。



## 总结

在本文的案例中，我们从不完善到完善，以渐进优化的方式，跟大家介绍了电商订单数据的分库分表解决方案。我们也可以从中看到，对于当前技术方案进行迭代升级的思考和选型取舍。

在下一篇文章中，我们会继续以这样的方式，跟大家介绍一个业务场景更加复杂的分库分表解决方案。